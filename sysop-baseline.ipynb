{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "data_file = 'sysop-baseline-uniq.csv'\n",
    "num_data_per_app = 30\n",
    "\n",
    "run_name = \"mpnet-base-all-nli-triplet\"\n",
    "base_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "output_model_dir = f\"models/{run_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy to load csv file, the csv file contains single column of data with header\n",
    "def load_csv_file(file_path):\n",
    "    data = np.genfromtxt(\n",
    "        file_path,\n",
    "        delimiter='\\t',\n",
    "        dtype=str)\n",
    "    return data\n",
    "\n",
    "data = load_csv_file(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrac dismhost.exe in temp\n",
    "# https://answers.microsoft.com/en-us/windows/forum/all/what-is-dismhostexe-in-temp-folder/7fa5bcf5-bfaf-4952-b05d-cec0a57461e3\n",
    "\n",
    "# extract google\\chrome\\application\n",
    "# extract microsoft\\edgewebview\\application\n",
    "# extract microsoft\\edgeupdate\n",
    "# extract rockwell software\n",
    "# extract national instruments\n",
    "\n",
    "filters = [\n",
    "    'dismhost.exe',\n",
    "    'google\\\\chrome\\\\application',\n",
    "    'microsoft\\\\edgewebview\\\\application',\n",
    "    'microsoft\\\\edgeupdate',\n",
    "    'rockwell software',\n",
    "    'national instruments'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by the filtered data, create a map of filtered data\n",
    "# map key is filters, map value is the filtered data\n",
    "filtered_data_map = {}\n",
    "for filter in filters:\n",
    "    # find all rows that contains the filter\n",
    "    filter_data = data[np.char.find(data, filter) != -1]\n",
    "    # append the filtered data to the filtered_data\n",
    "    filtered_data_map[filter] = filter_data\n",
    "\n",
    "# only keep the top 30 data in the filtered data\n",
    "for key in filtered_data_map:\n",
    "    filtered_data_map[key] = filtered_data_map[key][:num_data_per_app]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = SentenceTransformer(base_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings per filter\n",
    "app_embeddings = {}\n",
    "details = []\n",
    "for app, v in filtered_data_map.items():\n",
    "    app_embeddings[app] = model.encode(v)\n",
    "    for data in v:\n",
    "        # keep last 50 characters\n",
    "        details.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine embeddings in single array\n",
    "embeddings = np.vstack((app_embeddings[filters[0]], app_embeddings[filters[1]]))\n",
    "for app in filters[2:]:\n",
    "    embeddings = np.vstack((embeddings, app_embeddings[app]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TSNE to reduce to 3 components\n",
    "tsne_model = TSNE(n_components=3, random_state=42)\n",
    "tsne_embeddings_values = tsne_model.fit_transform(embeddings)\n",
    "\n",
    "hover_names = details\n",
    "colors = [filter for filter in filters for _ in range(num_data_per_app)]\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x = tsne_embeddings_values[:,0],\n",
    "    y = tsne_embeddings_values[:,1],\n",
    "    z = tsne_embeddings_values[:,2],\n",
    "    hover_name=hover_names,\n",
    "    color = colors,\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=13))  # Increase the marker size uniformly\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(showticklabels=False, title=''),\n",
    "        yaxis=dict(showticklabels=False, title=''),\n",
    "        zaxis=dict(showticklabels=False, title=''),\n",
    "    ),\n",
    "    #showlegend=False,\n",
    "    autosize=True,\n",
    "    #width=600,  # Width of the plot\n",
    "    #height=600,  # Height of the plot\n",
    "    margin=dict(l=50, r=50, b=50, t=50, pad=4)  # Margins\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a triplet data\n",
    "# triplet data is a tuple of 3 data, the first data is the anchor, the second data is the positive, and the third data is the negative\n",
    "# the anchor and positive are from the same application, the negative is from different application\n",
    "triplet_data = []\n",
    "for filter in filters:\n",
    "    app_data = filtered_data_map[filter]\n",
    "    for i in range(num_data_per_app):\n",
    "        anchor = app_data[i]\n",
    "        positive = app_data[(i + 1) % num_data_per_app]\n",
    "        for filter2 in filters:\n",
    "            if filter2 != filter:\n",
    "                negative = filtered_data_map[filter2][i]\n",
    "                triplet_data.append((anchor, positive, negative))\n",
    "\n",
    "# shuffle the triplet data\n",
    "np.random.shuffle(triplet_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train, validation, and test\n",
    "train_data = triplet_data[:int(len(triplet_data) * 0.8)]\n",
    "validation_data = triplet_data[int(len(triplet_data) * 0.8):int(len(triplet_data) * 0.9)]\n",
    "test_data = triplet_data[int(len(triplet_data) * 0.9):]\n",
    "\n",
    "# convert the triplet data into dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [data[0] for data in train_data],\n",
    "    \"positive\": [data[1] for data in train_data],\n",
    "    \"negative\": [data[2] for data in train_data],\n",
    "})\n",
    "validation_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [data[0] for data in validation_data],\n",
    "    \"positive\": [data[1] for data in validation_data],\n",
    "    \"negative\": [data[2] for data in validation_data],\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"anchor\": [data[0] for data in test_data],\n",
    "    \"positive\": [data[1] for data in test_data],\n",
    "    \"negative\": [data[2] for data in test_data],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function\n",
    "loss = MultipleNegativesRankingLoss(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=output_model_dir,\n",
    "\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    # If per_device_train_batch_size 8 and you are using 2 GPUs,\n",
    "    # each GPU will process 8 samples per batch, resulting in a total batch size of 16 across all devices.\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    fp16=False,  # Set to False if GPU can't handle FP16\n",
    "    bf16=True,  # Set to True if GPU supports BF16\n",
    "\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicates\n",
    "\n",
    "    use_mps_device=True,\n",
    "\n",
    "    # Optional tracking/debugging parameters:\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "\n",
    "    # The checkpoint save strategy to adopt during training.\n",
    "    # ”no”: No save is done during training.\n",
    "    # ”epoch”: Save is done at the end of each epoch.\n",
    "    # ”steps”: Save is done every save_steps.\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    logging_steps=100,\n",
    "\n",
    "    run_name=run_name,  # Used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Create an evaluator & evaluate the base model\n",
    "val_evaluator = TripletEvaluator(\n",
    "    anchors=validation_dataset[\"anchor\"],\n",
    "    positives=validation_dataset[\"positive\"],\n",
    "    negatives=validation_dataset[\"negative\"],\n",
    "    name=\"validation\",\n",
    ")\n",
    "val_evaluator(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=val_evaluator,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Evaluate the trained model on the test set, after training completes\n",
    "test_evaluator = TripletEvaluator(\n",
    "    anchors=test_dataset[\"anchor\"],\n",
    "    positives=test_dataset[\"positive\"],\n",
    "    negatives=test_dataset[\"negative\"],\n",
    "    name=\"test\",\n",
    ")\n",
    "test_evaluator(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings per filter\n",
    "app_embeddings = {}\n",
    "details = []\n",
    "for app, v in filtered_data_map.items():\n",
    "    app_embeddings[app] = model.encode(v)\n",
    "    for data in v:\n",
    "        # keep last 50 characters\n",
    "        details.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine embeddings in single array\n",
    "embeddings = np.vstack((app_embeddings[filters[0]], app_embeddings[filters[1]]))\n",
    "for app in filters[2:]:\n",
    "    embeddings = np.vstack((embeddings, app_embeddings[app]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TSNE to reduce to 3 components\n",
    "tsne_model = TSNE(n_components=3, random_state=42)\n",
    "tsne_embeddings_values = tsne_model.fit_transform(embeddings)\n",
    "\n",
    "hover_names = details\n",
    "colors = [filter for filter in filters for _ in range(num_data_per_app)]\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x = tsne_embeddings_values[:,0],\n",
    "    y = tsne_embeddings_values[:,1],\n",
    "    z = tsne_embeddings_values[:,2],\n",
    "    hover_name=hover_names,\n",
    "    color = colors,\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=13))  # Increase the marker size uniformly\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(showticklabels=False, title=''),\n",
    "        yaxis=dict(showticklabels=False, title=''),\n",
    "        zaxis=dict(showticklabels=False, title=''),\n",
    "    ),\n",
    "    #showlegend=False,\n",
    "    autosize=True,\n",
    "    #width=600,  # Width of the plot\n",
    "    #height=600,  # Height of the plot\n",
    "    margin=dict(l=50, r=50, b=50, t=50, pad=4)  # Margins\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
